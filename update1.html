<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Painting Detector
    | CS, Georgia Tech | Fall 2018: CS 4476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
  <style>
    body {
      padding-top: 60px;
      /* 60px to make the container go all the way to the bottom of the topbar */
    }

    .vis {
      color: #3366CC;
    }

    .data {
      color: #FF9900;
    }
  </style>

  <link href="css/bootstrap-responsive.min.css" rel="stylesheet">

  <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
  <div class="container">
    <div class="page-header">

      <!-- Title and Name -->
      <h1>Painting Detector - Project Update 1</h1>
      <span style="font-size: 20px; line-height: 1.5em;"><strong>Akhila Ballari, Hemanth Bellala, Raghav Bhat, Carl
          Mubarak, Vivekanand Rajasekar</strong></span><br>
      <span style="font-size: 20px; line-height: 1.5em;"><strong>aballari6, hbellala3, rbhat35, cmubarak3, vrajasekar6</strong></span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Fall 2018 CS 4476 Computer Vision: Painting Detector</span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
      <hr>
      <br><br>


      <!-- Introduction -->
      <h3>Abstract</h3>

      <------------>
      Abstract goes here
      <------------>

      <br><br>


      <!-- Teasure Figure -->
      <h3>Teaser Figure</h3>

      <------------>
      Teaser Figure
      <------------>

      <br><br>


      <!-- Introduction -->
      <h3>Introduction</h3>

      This project will demonstrate a system that uses computer vision techniques in
      order to help museum goers learn about the paintings they see. The system will
      work by having users take photos of—or with—the paintings they like or want to
      learn more about. Our system will then take as input these images and, using a
      variety of known-techniques, find the painting’s match if it exists in the
      database. The output of the system will be the original painting, as well as
      information detailing the painting—it’s creator and a description of painting.
      The authors of this paper believe that this will be a valuable and important
      tool, allowing museum visitors to focus on seeing art while they’re at the
      museum and learning more about their favorite pieces at home. In order to label
      these paintings well, we will analyze several different computer vision
      approaches, focusing on efficiency and accuracy.

      <br><br>


      <!-- Approach -->
      <h3>Approach</h3>
      We have identified several approaches we will use to detect paintings—texture
      analysis, edge detection, color analysis, and feature detection. We shall first
      tune each of these different approaches to the images. Then, we shall use a
      combination of these approaches to find an efficient algorithm that labels as
      many paintings correctly as possible.

      <br><br>

      <u>Method 1 - Texture</u>

      <br><br>
      Texture analysis will improve the overall result of our system since different
      paintings may have similar colors and features, but may be of two different
      mediums. For example, oils paints are thicker than watercolor paints. Even though
      an oil painting of a city landscape contains the same features and colors as a
      watercolor painting of the same city landscape, the texture of the oil painting
      may be more rugged than the watercolor paintings. <br><br>
      We plan on running a set of diverse filters on every pixel of each image in our
      database, then concatenating the filtered values together to form an
      M x N x k matrix, where k is the number of filters and M, N are the dimensions
      of the image. We will have one M x N x k matrix for every image in our dataset.
      When an image is inputted into our system, we will apply each filter in our
      filter bank to produce an M x N x k matrix for the input. We will then compute
      the sum of the squared distances to find the painting in our dataset with the
      smallest euclidean distance to our input, which will be the most
      similar-painting. Rather than implementing this process ourselves, we plan to
      use OpenCV and OpenGL. <br><br>
      The authors believe that this approach individually may not give us the result
      we desire due to the difficulty in capturing the texture of a painting through
      a picture of it: several factors, such as lighting, may obscure the textures.
      However, it could be used in narrowing down on possible matches.

      <br><br>
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="texture.jpg">
        <a href="http://art-now-and-then.blogspot.com/2014/03/choosing-painting-medium.html">Image Credits</a>
      </div>

      <br><br>

      <u>Method 2 - Edges</u>

      <br><br>
      Edge detection will aid in narrowing down on the painting style or time period that the painting originated from.
      Paintings from different time periods vary in edge style. While modernist paintings incorporate more structured
      and prominent edges, paintings from eras such as Impressionism use softer and less prominent edges. We will
      utilize Canny Edge Detection to analyze these paintings as it aids in detecting a wide range of edges.
      The dataset we are using is categorized by time period, so we plan to preprocess a sample of paintings from each
      time period to determine the period’s edge characteristics. Then for each input image, we can use Canny Edge
      Detection to find the closest match. We will have to be careful with the threshold used in Canny Edge Detection,
      as we do not want to completely loose any soft edge information. The threshold will have to be low enough to be
      able to detect soft edges, so the intensity of each detected edge can help accurately identify edge
      characteristics and ultimately the time period that the painting could have originated from. We will utilize
      OpenCV for the Canny Edge Detection. While this approach will not guarantee an exact match for a given input
      image, it will aid in narrowing down on specific time period that the painting could have belonged to.

      <br><br>
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="edge.jpg">
        <a href="http://image.ntua.gr/iva/tools/mfd/">Image Credits</a>
      </div>

      <br><br>

      <u>Method 3 - Color</u>

      <br><br>
      Another approach that we will be looking at is analyzing the colors in the image to classify the paintings. A
      basic approach might entail simply summing up the pixel values for each of the R, G, and B channels and looking
      for images that have similar intensity values. A slightly more robust solution might entail a histogram of pixel
      color combinations and matching the input image with other pictures that have similar histograms of the pixel
      colors. However, a much more robust classification based on the colors of the images could be achieved using
      neural networks. If we train our classifier to remember the color values of the pixels given a set of images,
      then, based on an input image’s color values, we can predict which of the image this is coming from. We can use
      methods in scikit to implement the neural network, and certain methods in OpenCV to derive the color values of an
      image.

      <a href="http://www.primaryobjects.com/2013/06/17/an-intelligent-approach-to-image-classification-by-color/">View
        Source</a>

      <br><br>
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="color.png"></div>
        <a href="https://blog.asmartbear.com/color-wheels.html">Image Credits</a>
      <br><br>

      <u>Method 4 - Features</u>

      <br><br>
      We plan to use feature detection as another part of our approach for scoring paintings to see which one is our
      target painting. Using this, we can easily track specific patterns and features between our sample photos and the
      target paintings. Features will be areas with the maximum variation when moved in all the regions around it.
      Feature detection can be done using either the Harris corner detection or blob detection. Harris corner detection
      is a very simple yet efficient algorithm which finds the corners, which are the parts of an image least affected
      by translation, rotation, and illumination, of the images to effectively find matches between images. Blob
      detection follows the same concept of Harris corner detection, but instead of looking for corner it looks for a
      regions in a image that differ in properties relative to the regions’ surroundings, “blobs”. Blob detection is
      useful because the “blobs” that are found are fairly consistent across images. When looking to find matches
      between images, these “blobs” are very similar to corners in that they are not as affected by translation,
      rotation, and illumination. We will make use of available methods contained in the OpenCV library for this
      portion of our project.

      <br><br>
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="feature.png">
        <a href="http://image.ntua.gr/iva/tools/mfd/">Image Credits</a>
      </div>
      <br><br>


      <!-- Experiments and Results -->
      <h3>Experiments and results</h3>

      <u>Method 1 - Texture</u>
      <br><br>
      Experiments and Results for Texture
      <br><br>

      <u>Experiments and Results for Edge Analysis</u>
      <br><br>
      In order to test the accurancy of finding matches to an input image of a painting based on the edge structure of the image, we used a small sample of the images in the Wiki database. The sample of 9 images belong to different art styles, so they vary in edge quantity and structure. Some of the images in the sample set have very faint and soft edges, while others have more rigid and prominent egdes. <br><br>

      We proprocessed each image in the sample set by running the Canny Edge Detector on it with the most appropriate sigma for the Guassian Filter, and plotting an histogram of the values returned by the edge detector. Since skimage's canny edge detector uses white pixels to denote an edge on a black background, the histogram produced will only require 2 bins.
      The ratio of the number of white pixel to the the number of pixels in the entire gray image would provide a good guage on the promience of edges in each image. So, we determined and stored the white pixel ratio for each image. The image below shows the histogram for the given image. As we can see the tomb image doesn't have too many promienent edges, so the first bin with a pixel value of 0 (black pixels) has a higher frequency, than the second bin with a pixel value of 1 (white pixels). <br><br>

      <br><br>
      <div style="text-align: center;">
        <img style="height: 400px;" alt="" src="images/results/tombHist.jpg">
      </div>
      <br><br>

      For any input image and a specified sigma, for which we want to find a match, we processed the image in the same way. We ran skimage's canny edge detecor on the input image, and determined the white pixel ratio by plotting a histogram of the returned edges array. The sigma for an input image is a free parameter that determines the accuracy of the results. We
      determined the matches to the input image by finding all the images in the sample set that have a white pixel ratio that has a percent difference respective to the input image's white pixel ratio that is less than the specified threshold. For the experiment, this threshold was set to 10%. (Percent difference of the ratio of each image in the same and the ratio of the imput imgae should be less than 10% for the individual image to be considered a match.)<br><br>

      This approach seems to work really well in distinguishing images with softer and less prominent edges from images with bolder edges. With an appropriate sigma, we were able to narrow down the set to the input image if the input image contained soft edges. The image on the left is the result of finding a match of the 'tomb' image with a sigma of 1.7. (The original image was preprocessed with a sigma of 1.8). As we can see from the image on the right, this approach also works really well with the 'john-ferren' image with a sigma of 0.55. (The original image was preprocessed with a sigma of 0.5).

      <div style="text-align: center;">
        <img style="height: 400px;" alt="" src="images/results/tombResults.jpg">
        <img style="height: 400px;" alt="" src="images/results/john-ferrenResults.jpg">
      </div>

      <br><br>
      The value of these sigmas for the softer edges images drastically affected the results of the detector based on edges since blurring the image too much or too less could either take out too much of the important edges or introduce too much noise. For example, running the detector on the 'tomb' image with a sigma of 1.5 does not produce any matches at all. Using only edges as the macthing factor in the detector does not work as well in distinguishing amongst images with very promienent and rigid edegs. As we can see from the images below, while we were able to get the right image back within the matches, we also get matches with other non-related images. The first image on the left shows the results for running the detector on the 'abstract' image with a sigma of 1.5 (preprocessed with a sigma of 2), and the second image on the right shows the results for running the detector on the 'synchromy-in-orange' image with a sigma of 1.2 (preprocessed with a sigma of 1.3).

      <br><br>
      <div style="text-align: center;">
        <img style="height: 400px;" alt="" src="images/results/abstractResults.jpg">
        <img style="height: 400px;" alt="" src="images/results/synchromy-in-orangeResults.jpg">
      </div>

      The value of the sigma is not as significant for these images, since the images would already have a lot of noise. Perhaps lowering the threshold, so that the percent difference is tightented, would increase the accuracy.
      <br><br>

      <u>Method 3 - Color</u>
      <h5> Dataset </h5>
      For the midterm update, the color has only been updated on a smaller subset of the full dataset. We are testing it with these 4 images for now to analyze the results closely before we move to the full dataset. The first image serves as the query.

      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="color/images/monaLisa_museum.jpg">
        <img style="height: 200px;" alt="" src="color/images/monaLisa_original.jpg">
        <img style="height: 200px;" alt="" src="color/images/monaLisa_orig2.jpg">
        <img style="height: 200px;" alt="" src="color/images/starryNight.jpg">
      </div>

      <h5> Histogram Differences </h5>
        <ol>
        <li>In this approach, we first generate a histogram for each of the images in our dataset. We are using 8 bins for each of the RGB channels and allocate each pixel to three bins.</li>

        <li>Given a query image, we generate a histogram for this image</li>
        <li>We compare the two histograms using these 5 methodologies:
          <ol>
            <li>
                Correlation
            </li>
            <li>
                Chi-Squared Distance
            </li>
            <li>
              Intersection
            </li>
            <li>
                Bhattacharyya Distance
            </li>
            <li>
                Hellinger Distance
            </li>
          </ol>
        </li>
        <li>
        We can then analyse the different scores of the system for each of the methodologies.
        </li>
        </ol>

      <div style="text-align: center;">
        Museum
        <img style="height: 200px;" alt="" src="color/images/channel_images/museum_b.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/museum_g.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/museum_r.png">
      </div>

      <div style="text-align: center;">
      Original
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig_b.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig_g.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig_r.png">
      </div>

      <div style="text-align: center;">
      Gradient Match
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig2_b.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig2_g.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig2_r.png">
      </div>

      <div style="text-align: center;">
      Dissimilar
        <img style="height: 200px;" alt="" src="color/images/channel_images/starry_b.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/starry_g.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/starry_r.png">
      </div>



      <table style="width:100%">
        <tr>
          <th>Image</th>
          <th>Correlation</th>
          <th>Chi-Squared</th>
          <th>Intersection</th>
          <th>Bhattacharyya</th>
          <th>Hellinger</th>
        </tr>
        <tr>
          <td>Query</td>
          <td>1.00</td>
          <td>0.00</td>
          <td>4.17</td>
          <td>1.00</td>
          <td>1.00</td>
        </tr>
        <tr>
          <td>Original</td>
          <td>.28</td>
          <td>-532.78</td>
          <td>1.50</td>
          <td>0.36</td>
          <td>0.36</td>
        </tr>
        <tr>
          <td>Similar Gradients</td>
          <td>.22</td>
          <td>-57.97</td>
          <td>1.02</td>
          <td>0.28</td>
          <td>0.28</td>
        </tr>
        <tr>
          <td>Starkly Different</td>
          <td>.06</td>
          <td>-80.90</td>
          <td>0.41</td>
          <td>0.12</td>
          <td>0.12</td>
        </tr>
      </table>

      <br><br>

      As we can see here, the query performs best with all histogram comparison methods. This makes sense intuitively since the histograms produced should be the same, since the pictures are the same.<br><br>

      Correlation seems to do pretty well since there is a significant difference between the Mona Lisa paintings and the Starry Nights. However, my issue with this method is that .30 correlation is not high enough.<br><br>

      Chi-Squared is producing a result quite on the contrary to what we expect and desire. The most favoriable outcome is actually the painting in a different color context than the original. And the starkly different painting is favored over the original painting present in the museum picture. <br><br>

      Intersection seems to provide the most meaningful and useful results. It seems as though we can produce scores over 1 when the images are relatively close and below when there is very little in common. <br><br>

      Bhattacharyya and Hellinger are actually the same formula (as implemented in opencv), and they produce results very similar to those from correlation. Due to the low scores for high matches, we must discard these results as well. <br><br>

      Intersection seems to produce the highest results. Next parameters that we can tune are the bin size. We also need to run this with a larger data set to determine what a high enough intersection score will determine similarity and what score indicates a clear distinction between two paintings.<br><br>

      <h5>Qualitative Results for Color</h5>
      <div style="text-align: center;">
        <img style="height: 350px;" alt="" src="color/images/Qualitative/Correlation.png">
        <img style="height: 350px;" alt="" src="color/images/Qualitative/Chi-Squared.png">
        <img style="height: 300px;" alt="" src="color/images/Qualitative/Hellinger.png">
        <img style="height: 350px;" alt="" src="color/images/Qualitative/Intersection.png">
      </div>

      <br><br>

      <u>Method 4 - Features</u>
      <br><br>
      Experiments and Results for Features
      <br><br>


      <!-- Conclusion and Future Work -->
      <h3>Conclusion and Future Work</h3>

      <br><br>
      Finding matches utilizing edges does help narrow down on a set of images. This approach works the best when distinguishing images with soft edges with images with hard edges. However, it cannot be solely used for distinguish amongst images with similar hard edges. The free parameter sigma is used to control how much to blur an input image and significantly
      affects the the accuracy at which the images with soft edges are detected. Since this is an easier parameter to control than the threshold values for maximum supression, we decided to use skimage's canny edge detector instead of OpenCV's canny edge detector. This approach processed the small sample set and found matches relatively fast. We will have to test the performance of this approach on a significantly larger data set of images with a greater variety of edges. We will also explore if we can get more information other than the white pixel to total pixel ratio from the canny edge detector. Finding matches though edge analysis is definitely impactfull and will be a significant componenent of the final painting detector.

      <br><br>
      We have only run one of the three color algorithms that was proposed. We
      still need to try the k-means clustering algorithm as a way of determining
      whether 2 paintings map to the same clusters or not. We also still need to
      implement the method of using neural nets to remember the color compositions of various paintings and their artists. We also need to expand the dataset size from 4 images to the full data set of 1000s of images. We need to check whether or not the intersection is optimal in
      that case as well. Although this will take significant amounts of time to run, the results should reveal the effectiveness of color as a means of
      classifying paintings. This will also allow our Neural net to be effective. As far as the parameters are concerned, we still need to figure out what the best bin-size is that optimizes the scores generated.
      <br><br>


      <!-- References -->
      <h3>References</h3>

      “How-To: 3 Ways to Compare Histograms Using OpenCV and Python.” PyImageSearch, 2 Aug. 2018, www.pyimagesearch.com/2014/07/14/3-ways-compare-histograms-using-opencv-python/

      <br><br>
      <!-- Links to Updates -->
      <h3>Check out our Proposal, Updates, and Results!</h3>

      <a href="index.html">Project Proposal</a>
      <br><br>

      <hr>
      <footer>
        <p>© Akhila Ballari, Hemanth Bellala, Raghav Bhat, Carl Mubarak, Vivekanand Rajasekar</p>
      </footer>
    </div>
  </div>

  <br><br>
</body>

</html>
