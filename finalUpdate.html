<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Painting Detector
    | CS, Georgia Tech | Fall 2018: CS 4476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
  <style>
    body {
      padding-top: 60px;
      /* 60px to make the container go all the way to the bottom of the topbar */
    }

    .vis {
      color: #3366CC;
    }

    .data {
      color: #FF9900;
    }
  </style>

  <link href="css/bootstrap-responsive.min.css" rel="stylesheet">

  <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
  <div class="container">
    <div class="page-header">

      <!-- Title and Name -->
      <h1>Painting Detector - Final Project Update</h1>
      <span style="font-size: 20px; line-height: 1.5em;"><strong>Akhila Ballari, Hemanth Bellala, Raghav Bhat, Carl
          Mubarak, Vivekanand Rajasekar</strong></span><br>
      <span style="font-size: 20px; line-height: 1.5em;"><strong>aballari6, hbellala3, rbhat35, cmubarak3, vrajasekar6</strong></span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Fall 2018 CS 4476 Computer Vision: Painting Detector</span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
      <hr>
      <br><br>


      <!-- Introduction -->
      <h3>Abstract</h3>
      <br><br>
      The motivation for this project is to figure out the combination of different detection methods which best match an input image to its match within a database of paintings. The application will be used my museum-goers to look up and learn more about the paintings they liked during their museum visit. Initially, we implemented and tested several matching algorithms individually to test their accuracy on image matching. From there, we have combined the detection methods in order to maximize the number of query images that match their result image, while also considering query speed. Our final program achieves 100% query accuracy on our test set when querying against a diverse dataset, and does so in XXX seconds.
      <br><br>

      <br><br>


      <!-- Teasure Figure -->
      <h3>Teaser Figure</h3>
      <br><br>
      Our first image would be the input, and our ideal output would be the actual painting with artist name and painting description. Below is an example with the Mona Lisa.
      <br><br>
      <div style="text-align: center;">
        <img style="height: 400px;" alt="" src="monaLisaPhoto.jpg">
        <img style="height: 400px;" alt="" src="monaLisa.jpg">
        <br><br>
        Painting: Mona Lisa
        <br><br>
        Artist: Leonardo da Vinci
        <br><br>
        Date: 1504
        <br><br>
        Style: High Renaissance
        <br><br>

      </div>
      <br><br>

      <br><br>


      <!-- Introduction -->
      <h3>Introduction</h3>

      This project will demonstrate a system that uses computer vision techniques in
      order to help museum goers learn about the paintings they see. The system will
      work by having users take photos of—or with—the paintings they like or want to
      learn more about. Our system will then take as input these images and, using a
      variety of known-techniques, find the painting’s match if it exists in the
      database. The output of the system will be the original painting, as well as
      information detailing the painting—it’s creator and a description of painting.
      The authors of this paper believe that this will be a valuable and important
      tool, allowing museum visitors to focus on seeing art while they’re at the
      museum and learning more about their favorite pieces at home. In order to label
      these paintings well, we will analyze several different computer vision
      approaches, focusing on efficiency and accuracy.

      <br><br>


      <!-- Approach -->
      <h3>Approach</h3>
      We have identified several approaches we will use to detect paintings—texture
      analysis, edge detection, color analysis, and feature detection. We shall first
      tune each of these different approaches to the images. Then, we shall use a
      combination of these approaches to find an efficient algorithm that labels as
      many paintings correctly as possible.

      <br><br>

      <u>Method 1 - Texture</u>

      <br><br>
      We believe texture analysis will improve the overall result of our system since different paintings may have similar colors and features but may be of two different mediums. For example, oils paints are thicker than watercolor paints. Even though an oil painting of a city landscape contains the same features and colors as a watercolor painting of the same city landscape, the texture of the oil painting may be more rugged than the watercolor paintings. <br><br>

      We will use local binary patterns texture matching, a well-studied technique that is invariant to illumination as well as translation, the two primary ways our inputs will differ from our outputs. LBP matching works by forming an LBP descriptor, which is calculated by choosing n points on a circle of specified radius around a reference pixel and creating an n-bit number, where the ith bit is set to 1 if the ith value is greater than the reference pixel value, and 0 otherwise. This process is repeated for every pixel in the gray-scaled image, and thus the LBP descriptor is the same size as the original image. After the LBP descriptor has been created, a normalized histogram of the values is created. This process is repeated for every image in the training set. <br><br>

      In order to classify an image, the same process is repeated to create a normalized histogram of the LBP values. Then, a distance metric, such as chi-squared, is used to compare the test image histogram with the training data histograms. The most similar image—or the image with the lowest chi-squared error—is returned.
      <br><br>

      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="texture.jpg">
        <a href="http://art-now-and-then.blogspot.com/2014/03/choosing-painting-medium.html">Image Credits</a>
      </div>

      <br><br>

      <u>Method 2 - Edges</u>

      <br><br>
      Edge detection will aid in narrowing down on the painting style or time period that the painting originated from.
      Paintings from different time periods vary in edge style. While modernist paintings incorporate more structured
      and prominent edges, paintings from eras such as Impressionism use softer and less prominent edges. We will
      utilize Canny Edge Detection to analyze these paintings as it aids in detecting a wide range of edges.
      The dataset we are using is categorized by time period, so we plan to preprocess a sample of paintings from each
      time period to determine the period’s edge characteristics. Then for each input image, we can use Canny Edge
      Detection to find the closest match. We will have to be careful with the threshold used in Canny Edge Detection,
      as we do not want to completely loose any soft edge information. The threshold will have to be low enough to be
      able to detect soft edges, so the intensity of each detected edge can help accurately identify edge
      characteristics and ultimately the time period that the painting could have originated from. We will utilize
      OpenCV for the Canny Edge Detection. While this approach will not guarantee an exact match for a given input
      image, it will aid in narrowing down on specific time period that the painting could have belonged to.

      <br><br>
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="edge.jpg">
        <a href="http://image.ntua.gr/iva/tools/mfd/">Image Credits</a>
      </div>

      <br><br>

      <u>Method 3 - Color</u>

      <br><br>
      Another approach that we will be looking at is analyzing the colors in the image to classify the paintings. A
      basic approach might entail simply summing up the pixel values for each of the R, G, and B channels and looking
      for images that have similar intensity values. A slightly more robust solution might entail a histogram of pixel
      color combinations and matching the input image with other pictures that have similar histograms of the pixel
      colors. However, a much more robust classification based on the colors of the images could be achieved using
      neural networks. If we train our classifier to remember the color values of the pixels given a set of images,
      then, based on an input image’s color values, we can predict which of the image this is coming from. We can use
      methods in scikit to implement the neural network, and certain methods in OpenCV to derive the color values of an
      image.

      <a href="http://www.primaryobjects.com/2013/06/17/an-intelligent-approach-to-image-classification-by-color/">View
        Source</a>

      <br><br>
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="color.png"></div>
        <a href="https://blog.asmartbear.com/color-wheels.html">Image Credits</a>
      <br><br>

      <u>Method 4 - Features</u>

      <br><br>
      We plan to use feature matching as another part of our approach for scoring paintings to see which one is our
      target painting. Using this, we can easily track specific keypoints and descriptors between our sample photos and the
      target paintings. Features will be areas with the maximum variation when moved in all the regions around it. Using OpenCV's feature matching, we used a Fast Library for Approximate Nearest Neighbors (FLANN) based matcher to analyze the descriptors from the 2 images and match them up. Next, we filtered the matches so to eliminate outliers. Using the remaining matches, we could identify the features that matched between the two images.

      <br><br>
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="feature.png">
        <a href="http://image.ntua.gr/iva/tools/mfd/">Image Credits</a>
      </div>
      <br><br>


      <!-- Results -->
      <h3>Results from Individual Method Testing</h3>

      <u>Method 1 - Texture</u>
      <br><br>
      The implementation for this experiment was based on <a href="http://hanzratech.in/2015/05/30/local-binary-patterns.html">Bikramjot Hanzra’s Texture Matching using Local Binary Patterns</a> tutorial. The implementation uses several packages, including OpenCV and the local_binary_pattern algorithm in the skimage.feature package. The training dataset used was a collection of four famous paintings, all of which seem to have different textures. Two copies of each image were used in the training dataset in order to test this method’s sensitivity to angle, color intensity, and image size. As a pre-processing step, the images were rescaled to have the same number of pixels and dimensions. <br><br>

      The only user-defined parameter in the LBP algorithm is the radius of the circular patch. Since the purpose of this approach is to detect the fine-grained differences such as the medium of the painting—as opposed to coarser details like how thick lines tend to be in the painting—we iterated over several small values for radius ranging from 3 to 20 pixels. However, no radius value (including much larger ones) seemed to improve or drastically change the results of this technique. <br><br>

      As described in the approach section, in order to classify an input, we calculate its LBP descriptor and then use a distance metric—in this case, chi-squared—to compare the LBP descriptor against the training set. Our experiment returned the chi-squared error as well as the corresponding image. Below are examples of the results. <br><br>

      <div style="text-align: center;">
        <u>Query Image (left), Results w/chi-squared error (right). LBP radius = 15. </u>
        <br>
        <img style="height: 400px;" alt="" src="texture/results/dali_query15.png">
        <img style="height: 400px;" alt="" src="texture/results/dali_results15.png">
      </div>

      <br><br>
      <div style="text-align: center;">
        <u>Query Image (left), Results w/chi-squared error (right). LBP radius = 3. </u>
        <br>
        <img style="height: 400px;" alt="" src="texture/results/gogh_query3.png">
        <img style="height: 400px;" alt="" src="texture/results/gogh_results3.png">
      </div>

      The subsample of results above clearly show that this method did not work very well, regardless of the query image and the radius used. We believe that the algorithm is doing no better than guessing. For example, in the trial below with radius = 3, we see that one version of The Scream has a chi-squared error of 0.064 while the other has an error of 0.515 from our query image.

      <br><br>
      <div style="text-align: center;">
        <u>Query Image (left), Results w/chi-squared error (right). LBP radius = 3. </u>
        <br>
        <img style="height: 400px;" alt="" src="texture/results/scream_query3.png">
        <img style="height: 400px;" alt="" src="texture/results/scream_results3.png">
      </div>

      <br><br>

      <u>Method 2 - Edges</u>
      <br><br>
      In order to test the accurancy of finding matches to an input image of a painting based on the edge structure of the image, we used a small sample of the images in the Wiki database. The sample of 9 images belong to different art styles, so they vary in edge quantity and structure. Some of the images in the sample set have very faint and soft edges, while others have more rigid and prominent egdes. <br><br>

      We proprocessed each image in the sample set by running the Canny Edge Detector on it with the most appropriate sigma for the Guassian Filter, and plotting an histogram of the values returned by the edge detector. Since skimage's canny edge detector uses white pixels to denote an edge on a black background, the histogram produced will only require 2 bins.
      The ratio of the number of white pixel to the the number of pixels in the entire gray image would provide a good guage on the promience of edges in each image. So, we determined and stored the white pixel ratio for each image. The image below shows the histogram for the given image. As we can see the tomb image doesn't have too many promienent edges, so the first bin with a pixel value of 0 (black pixels) has a higher frequency, than the second bin with a pixel value of 1 (white pixels). <br><br>

      <br><br>
      <div style="text-align: center;">
        <img style="height: 400px;" alt="" src="images/results/tombHist.jpg">
      </div>
      <br><br>

      For any input image and a specified sigma, for which we want to find a match, we processed the image in the same way. We ran skimage's canny edge detecor on the input image, and determined the white pixel ratio by plotting a histogram of the returned edges array. The sigma for an input image is a free parameter that determines the accuracy of the results. We
      determined the matches to the input image by finding all the images in the sample set that have a white pixel ratio that has a percent difference respective to the input image's white pixel ratio that is less than the specified threshold. For the experiment, this threshold was set to 10%. (Percent difference of the ratio of each image in the same and the ratio of the imput imgae should be less than 10% for the individual image to be considered a match.)<br><br>

      This approach seems to work really well in distinguishing images with softer and less prominent edges from images with bolder edges. With an appropriate sigma, we were able to narrow down the set to the input image if the input image contained soft edges. The image on the left is the result of finding a match of the 'tomb' image with a sigma of 1.7. (The original image was preprocessed with a sigma of 1.8). As we can see from the image on the right, this approach also works really well with the 'john-ferren' image with a sigma of 0.55. (The original image was preprocessed with a sigma of 0.5).

      <div style="text-align: center;">
        <img style="height: 400px;" alt="" src="images/results/tombResults.jpg">
        <img style="height: 400px;" alt="" src="images/results/john-ferrenResults.jpg">
      </div>

      <br><br>
      The value of these sigmas for the softer edges images drastically affected the results of the detector based on edges since blurring the image too much or too less could either take out too much of the important edges or introduce too much noise. For example, running the detector on the 'tomb' image with a sigma of 1.5 does not produce any matches at all. Using only edges as the macthing factor in the detector does not work as well in distinguishing amongst images with very promienent and rigid edegs. As we can see from the images below, while we were able to get the right image back within the matches, we also get matches with other non-related images. The first image on the left shows the results for running the detector on the 'abstract' image with a sigma of 1.5 (preprocessed with a sigma of 2), and the second image on the right shows the results for running the detector on the 'synchromy-in-orange' image with a sigma of 1.2 (preprocessed with a sigma of 1.3).

      <br><br>
      <div style="text-align: center;">
        <img style="height: 400px;" alt="" src="images/results/abstractResults.jpg">
        <img style="height: 400px;" alt="" src="images/results/synchromy-in-orangeResults.jpg">
      </div>

      The value of the sigma is not as significant for these images, since the images would already have a lot of noise. Perhaps lowering the threshold, so that the percent difference is tightented, would increase the accuracy.
      <br><br>

      <u>Method 3 - Color</u>
      <h5> Dataset </h5>
      For the midterm update, the color has only been updated on a smaller subset of the full dataset. We are testing it with these 4 images for now to analyze the results closely before we move to the full dataset. The first image serves as the query.

      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="color/images/monaLisa_museum.jpg">
        <img style="height: 200px;" alt="" src="color/images/monaLisa_original.jpg">
        <img style="height: 200px;" alt="" src="color/images/monaLisa_orig2.jpg">
        <img style="height: 200px;" alt="" src="color/images/starryNight.jpg">
      </div>

      <h5> Histogram Differences </h5>
        <ol>
        <li>In this approach, we first generate a histogram for each of the images in our dataset. We are using 8 bins for each of the RGB channels and allocate each pixel to three bins.</li>

        <li>Given a query image, we generate a histogram for this image</li>
        <li>We compare the two histograms using these 5 methodologies:
          <ol>
            <li>
                Correlation
            </li>
            <li>
                Chi-Squared Distance
            </li>
            <li>
              Intersection
            </li>
            <li>
                Bhattacharyya Distance
            </li>
            <li>
                Hellinger Distance
            </li>
          </ol>
        </li>
        <li>
        We can then analyse the different scores of the system for each of the methodologies.
        </li>
        </ol>

      <div style="text-align: center;">
        Museum
        <img style="height: 200px;" alt="" src="color/images/channel_images/museum_b.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/museum_g.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/museum_r.png">
      </div>

      <div style="text-align: center;">
      Original
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig_b.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig_g.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig_r.png">
      </div>

      <div style="text-align: center;">
      Gradient Match
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig2_b.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig2_g.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/orig2_r.png">
      </div>

      <div style="text-align: center;">
      Dissimilar
        <img style="height: 200px;" alt="" src="color/images/channel_images/starry_b.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/starry_g.png">
        <img style="height: 200px;" alt="" src="color/images/channel_images/starry_r.png">
      </div>



      <table style="width:100%">
        <tr>
          <th>Image</th>
          <th>Correlation</th>
          <th>Chi-Squared</th>
          <th>Intersection</th>
          <th>Bhattacharyya</th>
          <th>Hellinger</th>
        </tr>
        <tr>
          <td>Query</td>
          <td>1.00</td>
          <td>0.00</td>
          <td>4.17</td>
          <td>1.00</td>
          <td>1.00</td>
        </tr>
        <tr>
          <td>Original</td>
          <td>.28</td>
          <td>-532.78</td>
          <td>1.50</td>
          <td>0.36</td>
          <td>0.36</td>
        </tr>
        <tr>
          <td>Similar Gradients</td>
          <td>.22</td>
          <td>-57.97</td>
          <td>1.02</td>
          <td>0.28</td>
          <td>0.28</td>
        </tr>
        <tr>
          <td>Starkly Different</td>
          <td>.06</td>
          <td>-80.90</td>
          <td>0.41</td>
          <td>0.12</td>
          <td>0.12</td>
        </tr>
      </table>

      <br><br>

      As we can see here, the query performs best with all histogram comparison methods. This makes sense intuitively since the histograms produced should be the same, since the pictures are the same.<br><br>

        Correlation seems to do pretty well since there is a significant difference between the Mona Lisa paintings and the Starry Nights. However, my issue with this method is that .30 correlation is not high enough.<br><br>

        Chi-Squared is producing a result quite on the contrary to what we expect and desire. The most favoriable outcome is actually the painting in a different color context than the original. And the starkly different painting is favored over the original painting present in the museum picture. <br><br>

        Intersection seems to provide the most meaningful and useful results. It seems as though we can produce scores over 1 when the images are relatively close and below when there is very little in common. <br><br>

        Bhattacharyya and Hellinger are actually the same formula (as implemented in opencv), and they produce results very similar to those from correlation. Due to the low scores for high matches, we must discard these results as well. <br><br>

        <h5>Qualitative Results for Color</h5>
        <div style="text-align: center;">
          <img style="height: 350px;" alt="" src="color/images/Qualitative/Correlation.png">
          <img style="height: 350px;" alt="" src="color/images/Qualitative/Chi-Squared.png">
          <img style="height: 300px;" alt="" src="color/images/Qualitative/Hellinger.png">
          <img style="height: 350px;" alt="" src="color/images/Qualitative/Intersection.png">
        </div>

        <br><br>

      <u>Method 4 - Features</u>
      <br><br>
      For the feature matching imeplementation, we used OpenCV's feature matching tools. First, we used SIFT to detect all the keypoints and their descriptors, and then used k-nearest neighbors to pair features from the photo of the painting to features of the set of available paintings that it could be. For this update, we used a subset of 5 paintings. Below are the results for identifying both the Mona Lisa and the Van Gough Self Portrait from 1889.
      <br><br>
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="featureMatchingResults/monaMatching1.png">
        <img style="height: 200px;" alt="" src="featureMatchingResults/monaMatching2.png">
        <img style="height: 200px;" alt="" src="featureMatchingResults/monaMatching3.png">
        <img style="height: 200px;" alt="" src="featureMatchingResults/monaMatching4.png">
        <img style="height: 200px;" alt="" src="featureMatchingResults/monaMatching5.png">
      </div>
      <br><br>
      <center><i>Matching the Mona Lisa against 5 images.</i></center>
      <br><br>

      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="featureMatchingResults/vgMatching1.png">
        <img style="height: 200px;" alt="" src="featureMatchingResults/vgMatching2.png">
        <img style="height: 200px;" alt="" src="featureMatchingResults/vgMatching3.png">
        <img style="height: 200px;" alt="" src="featureMatchingResults/vgMatching4.png">
        <img style="height: 200px;" alt="" src="featureMatchingResults/vgMatching5.png">
      </div>
      <br><br>
      <center><i>Matching Van Gough Self Portrait 1889 against 5 images.</i></center>
      <br><br>
      As we can see from the results above, there are many more matches found between the photo of the Mona Lisa and the actual painting itself. The actual numbers show that there are 34 matches with the Mona Lisa, and only 4, 2, 1, and 2 for the following 4 images respectively.

      <br><br>

      Intersection seems to produce the highest results. Next parameters that we can tune are the bin size. We also need to run this with a larger data set to determine what a high enough intersection score will determine similarity and what score indicates a clear distinction between two paintings.<br><br>


      <!-- Overall Experiments and Results -->
      <h3>Results of Complete Painting Detector</h3>

      <br><br>
      <u>Forming the Comprehensive Painting Detector</u>
      <br><br>  
      To tie in individually tested methods of image analysis (edge, color, and feature) into a comprehensive painting detector, we combined parts of the methods using a rating system. We collected a sample of 159 images from the WikiArt Database and randomly selected 5 query images. The sample contains images of the actual painting, while the query images are images of the paintings surrounded by a frame (as in a museum). Query Images were resized and compressed in order to speed up detection time.
      <br><br>

      As we found that edge detection accurately distinguished images based their edge structure, we used this method to construct a subset of the sample to aid feature and color detection in finding the right match faster. Through edge detection, we preprocessed all the images in sample and stored each image’s white pixel ratio using the Canny Edge Detector. For the passed in query image (manually cropped to exclude the frame and everything beyond it), we formed a subset of images with a percent difference in the white pixel ratio that is less than a specific threshold. Previously, the accuracy of the subset depended on the sigma specified for each image. Since setting a specific sigma to reduce noise for every individual image in a sample this large is not optimal, we decided to keep the threshold as a hyperparameter, tuning it to the passed in query image. The following results display the minimum threshold set for each query image and the resulting number of images in the subset that contains the right match formed. 
      <br><br>

      <table style="width:100%">
        <tr>
          <th>Query Image</th>
          <th>Threshold</th>
          <th>Subset Size</th>
        </tr>
        <tr>
          <td>The Scream</td>
          <td>0.20</td>
          <td>52</td>
        </tr>
        <tr>
          <td>Mona Lisa</td>
          <td>2.00</td>
          <td>159</td>
        </tr>
        <tr>
          <td>The Blind Guitarist</td>
          <td>0.50</td>
          <td>14</td>
        </tr>
        <tr>
          <td>Starry Night</td>
          <td>0.03</td>
          <td>15</td>
        </tr>
        <tr>
          <td>The Persistence of Memory</td>
          <td>0.40</td>
          <td>109</td>
        </tr>
      </table>

      <br><br>

      The optimality of the subset heavily depended on the query image since blurring was not used to take out extra noise. For example, the “mona-lisa” query image was significantly more blurry than it match in the sample images. It produced the least effective subset since the edge were not picked up as well. The most effective subsets were formed for “old-artist-chicago-picasso”, “starry-night”, and  “the-scream” since the images were of the same quality as their matches. The subsets for these images were reduced to less than about 30% of the original sample.  
      <br><br>
      After decreasing the subset of the images that need to analyzed with the edge detector we then ran feature and color detectors to efficiently get more accurate results. Based on previous experiments we saw that feature detection yielded much better results than using a color detector. So after we ran the algorithms on the smaller subset and  received a score for images for each algorithms, we had to weigh each of the scores differently. We weighed the feature detection with a weight of .7 and weighed the color detector with a weight of .3. Weighing the algorithms in this manor resulted in very accurate query results.  The implementation of each of the algorithms stayed the same as the last update, now we are running them together for more accurate results.
  
      <br><br>

      <!-- Qualitative Results -->
      <h3>Qualitative Results</h3>

      <br><br>
      The process of running our script is as follows: First, we load and display the query images, shown below. Then, the user clicks on the four corners of the painting (to yeild better results and exclude any irrelevant parts of the picture). Next, edge detection narrows down our possible matches to a subset using a threshold. Finally, the image is scored based on a weighted sum of feature and color detection scores (texture detection did not prove to give us helpful results). The resulting image is that with the highest score.
      <br><br>

      <center><i>Query Image: Mona Lisa.</i></center>
      <br><br>
      <div style="text-align: center;">
        <img style="height: 300px;" alt="" src="queryImages/mona-lisa.jpg">
        <img style="height: 300px;" alt="" src="queryResults/MonaResult.png">
      </div>
      <br><br>
      <center><i>Query Image: The Scream.</i></center>
      <br><br>
      <div style="text-align: center;">
        <img style="height: 300px;" alt="" src="queryImages/the-scream.jpg">
        <img style="height: 300px;" alt="" src="queryResults/ScreamResult.png">
      </div>
      <br><br>
      <center><i>Query Image: Persistence of Memory.</i></center>
      <br><br>
      <div style="text-align: center;">
        <img style="height: 300px;" alt="" src="queryImages/wall-clocks.jpg">
        <img style="height: 300px;" alt="" src="queryResults/MemoryResult.png">
      </div>
      <br><br>
      <center><i>Query Image: Starry Night.</i></center>
      <br><br>
      <div style="text-align: center;">
        <img style="height: 300px;" alt="" src="queryImages/starry-night.jpg">
        <img style="height: 300px;" alt="" src="queryResults/StarryResult.png">
      </div>
      <br><br>
      <center><i>Query Image: The Old Blind Guitarist.</i></center>
      <br><br>
      <div style="text-align: center;">
        <img style="height: 300px;" alt="" src="queryImages/old-artist-chicago-picasso.jpg">
        <img style="height: 300px;" alt="" src="queryResults/GuitaristResult.png">
      </div>
      <br><br>



      <!-- Conclusion and Future Work -->
      <h3>Conclusion and Future Work</h3>


      <br><br>
      Ruling out texture detection was a major surprise to our team. We imagined it originally to yield the most accurate results. Instead, we found that feature detection gave us the most accuracy, and so we weighted it as such. We were able to use edge detection to narrow down our image set, rather than use it to score images, as we originally planned. This change, however, seemed to help our results and drastically shorten our runtime.
      <br><br>
      The painting detector’s efficiently could be improved through properly setting appropriate sigma values during Canny Edge Detection for the images in the sample and setting the sigma for the query image as a hyperparameter. In practicality, setting these manually would not be useful. Training a neural net to output an appropriate sigma for an input image could aid in this process. While we were able to get very accurate results we can speed up this process by applying a binary search tree when locating the matched image. Right now we look at all the images when we try to find a match, but if we were to place these images in a graph we can cut down the search time drastically. We did not see a problem with run time for the small dataset that we are using, but if we were to use a huge data set, lookup time could be huge issue. If we were to expand this project, we would create .npy files containing all the information that could be gained from every image of the database to speed up painting recognition as much as possible. On a small scale, however, our system worked very well.
      <br><br>
      Overall, as a group, we are very happy with how our project turned out. We were able to tackle our problem statement and produce accurate, helpful results.
      <br><br>

      <!-- References -->
      <h3>References</h3>

      “How-To: 3 Ways to Compare Histograms Using OpenCV and Python.” PyImageSearch, 2 Aug. 2018, www.pyimagesearch.com/2014/07/14/3-ways-compare-histograms-using-opencv-python/

      <br><br>
      Texture Matching Using Local Binary Patterns (lbp), Opencv, Scikit-learn and Python
      http://hanzratech.in/2015/05/30/local-binary-patterns.html

      <br><br>
      <!-- Links to Updates -->
      <h3>Check out our Proposal, Updates, and Results!</h3>

      <a href="index.html">Project Proposal</a><br>
      <a href="update1.html">Project Update 1</a>
      <br><br>

      <hr>
      <footer>
        <p>© Akhila Ballari, Hemanth Bellala, Raghav Bhat, Carl Mubarak, Vivekanand Rajasekar</p>
      </footer>
    </div>
  </div>

  <br><br>
</body>

</html>
